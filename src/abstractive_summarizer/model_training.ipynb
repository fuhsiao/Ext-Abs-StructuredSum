{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8660301-5be7-4463-8fc8-bcf455738980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from model_utils import load_raw_data\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa04d032-64fc-4d37-bed5-be28e56b87b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples):\n",
    "    # tokenize input\n",
    "    inputs = [doc for doc in examples[\"extract\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    # tokenize label\n",
    "    labels = tokenizer(text_target=examples[\"abstract\"], max_length=max_target_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def compute_rouge(pred):\n",
    "    predictions, labels = pred\n",
    "    # decode Predictions\n",
    "    decode_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # decode labels\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decode_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)  \n",
    "    # compute results\n",
    "    res = metric.compute(predictions=decode_predictions, references=decode_labels, use_stemmer=True)\n",
    "    res = {key: value * 100 for key, value in res.items()}\n",
    "    # count generate token\n",
    "    pred_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions] \n",
    "    res['gen_len'] = np.mean(pred_lens)\n",
    "    return {k: round(v, 4) for k, v in res.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "152b061a-bf9d-4291-98fc-b3420809a270",
   "metadata": {},
   "source": [
    "#### Dataset, Pretrained Model, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e05ed9-5717-46ca-ab59-5c708a2f42a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setence pair path\n",
    "train_sentence_pair = '../../dataset/to_abstractive/train_pair.parquet'\n",
    "test_sentence_pair = '../../dataset/to_abstractive/test_pair.parquet'\n",
    "# loading metric, sentence pair\n",
    "metric = evaluate.load('rouge')\n",
    "raw_data = load_raw_data(train_sentence_pair, test_sentence_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2d129a-d88f-497b-90c6-8ab6b72c7375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading pretrained model, tokenizer\n",
    "model_checkpoint = \"facebook/bart-base\" # or use GanjinZero/biobart-v2-base\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "collator = DataCollatorForSeq2Seq(tokenizer, model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca848ff0-2445-434a-be2e-4523d725ea16",
   "metadata": {},
   "source": [
    "#### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf09278-2e49-436a-b014-fb01fa41c420",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 512\n",
    "tokenize_data = raw_data.map(preprocess_data, batched=True, remove_columns=['extract', 'abstract'])\n",
    "tokenize_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6028996c-9212-4bf4-99a3-3f4b8c80bde1",
   "metadata": {},
   "source": [
    "#### Seq2SeqArguments setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f4ade8-b772-4ac3-bf76-93abd43cdc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成超參數\n",
    "generation_config = {\n",
    "    'num_beams': 5,\n",
    "    'max_length': 512,\n",
    "    'min_length': 64,\n",
    "    'length_penalty': 2.0,\n",
    "    'early_stopping': True,\n",
    "    'no_repeat_ngram_size': 3\n",
    "    }\n",
    "\n",
    "model.config.update(generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655e296f-7c9e-493d-bf41-056eb8e310ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練超參數\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir='model/checkpoint_bart',\n",
    "    learning_rate=8e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=32,\n",
    "    per_device_eval_batch_size=1,\n",
    "    eval_accumulation_steps=64,\n",
    "    num_train_epochs=9,\n",
    "    # weight_decay=0.01,\n",
    "    lr_scheduler_type='linear',\n",
    "    warmup_ratio=0.1,\n",
    "    save_total_limit=9,\n",
    "    save_strategy='epoch',\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_steps=5625,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    seed=42,\n",
    "    log_level='error'\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4191c7c5-4523-4a52-9676-b83065fdf2aa",
   "metadata": {},
   "source": [
    "#### Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a5357e-9071-4b35-b1c0-22bef7a48866",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 初始化\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    args,\n",
    "    train_dataset=tokenize_data['train'],\n",
    "    eval_dataset=tokenize_data['validation'],\n",
    "    data_collator=collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_rouge\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35236465-f11d-45ed-bd4f-8b81ea729606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 開始訓練\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4de1713-b825-4b2a-9301-e31bbabacc06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33a1d9ad-76db-4178-80c3-2fceb3f8791d",
   "metadata": {},
   "source": [
    "#### Supplemental code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a35d16d4-888d-4a83-a44c-3cf7e3af8293",
   "metadata": {},
   "source": [
    "The maximum sequence limit of the current model is 1024, you can try the following methods to solve it  \n",
    "+ Loading LSG Model with larger input sequences  \n",
    "+ Using LSG Converter to extend the original model to longer sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89921984-64f3-45eb-8d73-3cab2ea2a699",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSG Model\n",
    "# model_checkpoint = \"ccdv/lsg-bart-base-4096\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, trust_remote_code = True, pass_global_tokens_to_decoder = True)\n",
    "\n",
    "## LSG Converter\n",
    "# from lsg_converter import LSGConverter\n",
    "# converter = LSGConverter(max_sequence_length=4096)\n",
    "# model, tokenizer = converter.convert_from_pretrained(\"GanjinZero/biobart-v2-base\", architecture=\"BartForConditionalGeneration\",\n",
    "#                                                      num_global_tokens=1,\n",
    "#                                                      block_size=128, sparse_block_size=128,\n",
    "#                                                      sparsity_factor=2, mask_first_token=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9c4a695-e104-4391-8c3a-cce0b1fc75f9",
   "metadata": {},
   "source": [
    "If the training is interrupted, you can train from the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5853e5da-851a-4d96-a4a8-1da303d9af1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume_from_checkpoint=True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_rapids",
   "language": "python",
   "name": "python3_rapids"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
